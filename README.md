# Retail Data Analytics Pipeline - Azure Data Engineering Project

## ğŸ¯ Project Overview

End-to-end data engineering pipeline built on Azure that processes retail transactions to generate daily purchase and revenue reports, implementing the **Medallion Architecture** (Bronze-Silver-Gold).

## ğŸ—ï¸ Architecture

```
REST API â†’ ADF â†’ ADLS (Bronze) â†’ PySpark (Silver) â†’ Aggregation (Gold) â†’ Synapse SQL â†’ Power BI/Reports
```

### Data Layers (Medallion Architecture)

- **ğŸ¥‰ Bronze (Raw Data)**: Raw data from REST API with no transformations
- **ğŸ¥ˆ Silver (Cleaned Data)**: Cleaned, filtered and validated data (purchase transactions only)
- **ğŸ¥‡ Gold (Aggregated Data)**: Aggregated data ready for consumption (daily reports)

## ğŸ› ï¸ Technology Stack

- **Azure Data Factory (ADF)**: Data orchestration and ingestion
- **Azure Data Lake Storage Gen2 (ADLS)**: Layered data storage
- **Azure Synapse Analytics**: PySpark processing and SQL Analytics
- **PySpark**: Data transformation and cleansing
- **Azure Synapse SQL Pool**: Queries and reporting
- **Python**: Processing scripts
- **Azure Key Vault**: Secrets management (optional)

## ğŸ“Š Business Use Case

**Client**: Retail Company
**Requirement**: Automated daily reports showing:
- Total purchases made
- Daily revenue generated
- Transaction trend analysis

## ğŸš€ Pipeline Components

### 1. Data Ingestion (Bronze Layer)
- Source: REST API with transactions
- Frequency: Daily
- Captured fields:
  - `customer_id`
  - `order_id`
  - `transaction_amount`
  - `transaction_type`
  - `transaction_date`
  - `product_id`

### 2. Transformation (Silver Layer)
- Data cleansing:
  - Null value removal
  - Data type validation
  - Transaction filtering (purchases only)
- Format standardization
- Deduplication

### 3. Aggregation (Gold Layer)
- Calculated metrics:
  - Total purchases per day
  - Total revenue per day
  - Unique transaction count
  - Unique customers per day

## ğŸ“ Project Structure

```
az-RetailDataAnalyticsPipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture-diagram.png
â”‚   â”œâ”€â”€ setup-guide.md
â”‚   â””â”€â”€ api-documentation.md
â”œâ”€â”€ adf/
â”‚   â”œâ”€â”€ pipelines/
â”‚   â””â”€â”€ datasets/
â”œâ”€â”€ synapse/
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”‚   â”œâ”€â”€ bronze-to-silver.ipynb
â”‚   â”‚   â””â”€â”€ silver-to-gold.ipynb
â”‚   â”œâ”€â”€ sql-scripts/
â”‚   â”‚   â””â”€â”€ create-gold-tables.sql
â”‚   â””â”€â”€ spark-jobs/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup-infrastructure.sh
â”‚   â””â”€â”€ generate-sample-data.py
â”œâ”€â”€ data-samples/
â”‚   â””â”€â”€ sample-transactions.json
â””â”€â”€ tests/
    â””â”€â”€ unit-tests/
```

## ğŸ”§ Pre-requisites

- Active Azure subscription
- Azure CLI installed
- Python 3.8+
- Basic knowledge of:
  - PySpark
  - SQL
  - Azure Portal

## ğŸ“¦ Installation and Setup

See [docs/setup-guide.md](docs/setup-guide.md) for detailed instructions.

## ğŸ“ Skills Demonstrated

- âœ… Data Lake architecture design and implementation
- âœ… ETL/ELT pipeline development
- âœ… PySpark for distributed processing
- âœ… Azure Data Factory orchestration
- âœ… Data quality and validation
- âœ… SQL analytics and reporting
- âœ… Medallion Architecture implementation
- âœ… Cloud cost optimization

## ğŸ“ˆ Expected Results

- Automated pipeline running daily
- Reporting time reduced from 4 hours to 30 minutes
- Data available for near-real-time analysis
- Scalability to process millions of transactions

## ğŸ”— Links

- [LinkedIn Profile](your-linkedin)
- [Portfolio Website](your-website)
- [Architecture Diagram](docs/architecture-diagram.png)

## ğŸ“„ License

This project is part of my professional portfolio.

---

**Developed by**: [Your Name]
**Date**: November 2025
**Contact**: [your-email]
