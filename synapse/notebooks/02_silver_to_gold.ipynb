{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver to Gold Layer Aggregation\n",
    "\n",
    "**Purpose:** Create business-ready daily revenue reports  \n",
    "**Input:** Silver parquet files (cleaned purchases)  \n",
    "**Output:** Gold parquet files (daily aggregations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Read Silver Layer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - UPDATE THIS\n",
    "STORAGE_ACCOUNT = \"stgretail1763452716\"\n",
    "SILVER_PATH = f\"abfss://retail@{STORAGE_ACCOUNT}.dfs.core.windows.net/silver/cleaned_transactions/*.parquet\"\n",
    "\n",
    "df_silver = spark.read.parquet(SILVER_PATH)\n",
    "\n",
    "print(f\"Silver records loaded: {df_silver.count()}\")\n",
    "df_silver.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Daily Revenue Aggregation\n",
    "\n",
    "Calculate key metrics:\n",
    "- Total revenue per day (sum of amounts)\n",
    "- Total purchases per day (count of transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily_revenue = (\n",
    "    df_silver\n",
    "    .groupBy(\"event_date\")\n",
    "    .agg(\n",
    "        sum(\"amount\").alias(\"daily_revenue\"),\n",
    "        count(\"*\").alias(\"total_purchases\")\n",
    "    )\n",
    "    .orderBy(\"event_date\")\n",
    ")\n",
    "\n",
    "print(\"Daily Revenue Report:\")\n",
    "df_daily_revenue.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Write to Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLD_PATH = f\"abfss://retail@{STORAGE_ACCOUNT}.dfs.core.windows.net/gold/daily_revenue\"\n",
    "\n",
    "df_daily_revenue.write.mode(\"overwrite\").parquet(GOLD_PATH)\n",
    "\n",
    "print(f\"âœ… Gold layer written to: {GOLD_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Summary Statistics (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, max, min\n",
    "\n",
    "summary = df_daily_revenue.agg(\n",
    "    sum(\"daily_revenue\").alias(\"total_revenue\"),\n",
    "    sum(\"total_purchases\").alias(\"total_purchases\"),\n",
    "    avg(\"daily_revenue\").alias(\"avg_daily_revenue\"),\n",
    "    max(\"daily_revenue\").alias(\"max_daily_revenue\"),\n",
    "    min(\"daily_revenue\").alias(\"min_daily_revenue\")\n",
    ")\n",
    "\n",
    "print(\"Overall Summary:\")\n",
    "summary.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "name": "pyspark"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
