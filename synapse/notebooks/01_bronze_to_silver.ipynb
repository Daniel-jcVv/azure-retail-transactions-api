{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bronze to Silver Layer Transformation\n",
    "\n",
    "**Purpose:** Clean and filter raw transaction data from Bronze layer  \n",
    "**Input:** Bronze parquet files (all transaction types)  \n",
    "**Output:** Silver parquet files (purchase transactions only, cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Read Bronze Layer Data\n",
    "\n",
    "Load raw data from Bronze container. Update the storage account name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - UPDATE THIS\n",
    "STORAGE_ACCOUNT = \"stgretail1763452716\"  # Your storage account\n",
    "BRONZE_PATH = f\"abfss://retail@{STORAGE_ACCOUNT}.dfs.core.windows.net/bronze/Daniel-jcVv/azure-retail-transactions-api/refs/heads/main/data-source/retail_transactions_bronze.parquet\"\n",
    "\n",
    "# Read Bronze data\n",
    "df_bronze = spark.read.parquet(BRONZE_PATH)\n",
    "\n",
    "print(f\"Bronze records loaded: {df_bronze.count()}\")\n",
    "df_bronze.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Transformation\n",
    "\n",
    "Apply business rules:\n",
    "- Filter only 'purchase' transactions\n",
    "- Remove null values in critical columns\n",
    "- Convert timestamp to date\n",
    "- Standardize payment method (lowercase)\n",
    "- Cast amount to proper type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver = (\n",
    "    df_bronze\n",
    "    .filter(col(\"event_type\") == \"purchase\")  # Only purchases\n",
    "    .dropna(subset=[\"customer_id\", \"amount\"])  # Remove nulls\n",
    "    .withColumn(\"event_date\", to_date(col(\"event_timestamp\")))  # Date conversion\n",
    "    .withColumn(\"payment_method\", lower(col(\"payment_method\")))  # Standardize\n",
    "    .withColumn(\"amount\", col(\"amount\").cast(\"float\"))  # Type casting\n",
    "    .select(\n",
    "        \"event_id\", \"customer_id\", \"event_date\", \"product_id\",\n",
    "        \"product_category\", \"payment_method\", \"amount\", \"location\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Silver records after transformation: {df_silver.count()}\")\n",
    "df_silver.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Write to Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SILVER_PATH = f\"abfss://retail@{STORAGE_ACCOUNT}.dfs.core.windows.net/silver/cleaned_transactions\"\n",
    "\n",
    "df_silver.write.mode(\"overwrite\").parquet(SILVER_PATH)\n",
    "\n",
    "print(f\"Silver layer written to: {SILVER_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "name": "pyspark"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
